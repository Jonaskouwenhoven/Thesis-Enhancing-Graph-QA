{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the results as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "df = pd.read_pickle(\"data/T500BaselineDual.pkl\")\n",
    "df = df[~df['Prompts'].str.contains(\"Manual\")]\n",
    "failed_rows, correct  = [], []\n",
    "for index, row in df.iterrows():\n",
    "    sexpgen = str(row['sexpgen'])\n",
    "\n",
    "    if \"VALUE\" in sexpgen:\n",
    "        correct.append(row)\n",
    "    else:\n",
    "        failed_rows.append(row)\n",
    "    \n",
    "df_failed = pd.DataFrame(failed_rows)\n",
    "df_correct = pd.DataFrame(correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table Exact Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table exact match is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5353185595567868"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "table_ids = []\n",
    "counter = 0\n",
    "for index, row in df_correct.iterrows():\n",
    "    predicted_Table = str(row['sexpgen']).split(\"(VALUE (\")[1].split(\" \")[0]\n",
    "    golden_table = row['table_id']\n",
    "\n",
    "    if predicted_Table == golden_table:\n",
    "        counter += 1\n",
    "\n",
    "print(\"The table exact match is\")\n",
    "counter / len(df_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure Exact Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The measure em is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3656509695290859"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Chcek the measures\n",
    "correct_measures = 0\n",
    "for index, row in df_correct[:].iterrows():\n",
    "    sexpgen = str(row['sexpgen'])\n",
    "\n",
    "    golden_measure = row['Measure'][0]\n",
    "\n",
    "    generated = sexpgen.split(\"MSR\")[1].split(\" \")[1]\n",
    "    # print(generated)\n",
    "    if golden_measure == generated:\n",
    "        correct_measures += 1\n",
    "\n",
    "print(\"The measure em is\")\n",
    "correct_measures / len(df_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rouge and blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-2: 0.578;\n",
      "BLEU: 70.802;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import evaluate\n",
    "\n",
    "\n",
    "\n",
    "to_skip_geo = ['Regio', 'RegioS', 'WijkenEnBuurten', 'Regiokenmerken']\n",
    "to_skip_time = ['Perioden', \"GN\"]\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "golden_sexps, generated_sexps = [], []\n",
    "for index, row in df_correct[:].iterrows():\n",
    "    # print(row['sexpgen'])\n",
    "    generated = str(row['sexpgen'])\n",
    "    ## GOlden Sexp\n",
    "    golden_Sexp = row['original_sexp']\n",
    "    # print(f\"\\n{generated}\\n\")\n",
    "\n",
    "    generated = generated.replace(\"RegioS\", \"Regio\")\n",
    "    for skip in to_skip_geo:\n",
    "        if skip in generated:\n",
    "            # print(\"Skipping\")\n",
    "            \n",
    "            generated = generated.replace(skip, \"GC\")\n",
    "\n",
    "    for skip in to_skip_time:\n",
    "        if skip in generated:\n",
    "            # print(\"Skipping\")\n",
    "            \n",
    "            generated = generated.replace(skip, \"TC\")\n",
    "    generated = re.sub(r'GC \\w+\\b', 'GC <GC>', generated)\n",
    "    generated = re.sub(r'TC \\w+\\b', 'TC <TC>', generated)\n",
    "\n",
    "    golden_Sexp = re.sub(r'GC \\w+\\b', 'GC <GC>', golden_Sexp)\n",
    "    golden_Sexp = re.sub(r'TC \\w+\\b', 'TC <TC>', golden_Sexp)\n",
    "\n",
    "    golden_sexps.append(golden_Sexp)\n",
    "    generated_sexps.append(generated)\n",
    "\n",
    "rouge_score = rouge.compute(predictions=generated_sexps, references=golden_sexps,\n",
    "                            rouge_types=[\"rouge2\"])[\"rouge2\"]\n",
    "bleu_score = bleu.compute(predictions=generated_sexps, references=golden_sexps)[\"score\"]\n",
    "\n",
    "print(f\"ROUGE-2: {round(rouge_score, 3)};\\n\"\n",
    "      f\"BLEU: {round(bleu_score, 3)};\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DIM F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.353"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "# Initialize an empty list to store the F1 scores for each comparison\n",
    "dim_f1 = []\n",
    "\n",
    "# Regex pattern to find dimension groups and their IDs\n",
    "pattern = r\"\\(DIM\\s+([^\\s]+)\\s+([A-Z0-9_]+)\\)\"\n",
    "\n",
    "for generated, golden in zip(generated_sexps, golden_sexps):\n",
    "    # Find all matches for generated and golden S-expressions\n",
    "    matches_generated = re.findall(pattern, generated)\n",
    "    matches_golden = re.findall(pattern, golden)\n",
    "\n",
    "    # Convert matches to sets of tuples for easy comparison\n",
    "    inf_dims = set(matches_generated)\n",
    "    target_dims = set(matches_golden)\n",
    "\n",
    "    # Calculate Precision, Recall, and F1 score\n",
    "    p = 0 if len(inf_dims) == 0 else len(inf_dims & target_dims) / len(inf_dims)\n",
    "    r = 1 if len(target_dims) == 0 else len(inf_dims & target_dims) / len(target_dims)\n",
    "    f1 = 0 if p + r == 0 else 2 * ((p * r) / (p + r))\n",
    "\n",
    "    # Append the calculated F1 score to the list\n",
    "    dim_f1.append(f1)\n",
    "\n",
    "# To see the F1 scores\n",
    "round(np.average(dim_f1), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Exact Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table score is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12673130193905818"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "table_ids = []\n",
    "counter = 0\n",
    "\n",
    "pattern = r\"\\(DIM\\s+([^\\s]+)\\s+([A-Z0-9_]+)\\)\"\n",
    "\n",
    "\n",
    "\n",
    "for index, row in df_correct.iterrows():\n",
    "    sexpgen = str(row['sexpgen'])\n",
    "\n",
    "    predicted_Table = str(row['sexpgen']).split(\"(VALUE (\")[1].split(\" \")[0]\n",
    "    golden_table = row['table_id']\n",
    "    golden_measure = row['Measure'][0]\n",
    "\n",
    "    generated = str(row['sexpgen'])\n",
    "    golden_Sexp = row['original_sexp']\n",
    "        \n",
    "    generated = re.sub(r'GC \\w+\\b', 'GC <GC>', generated)\n",
    "    generated = re.sub(r'TC \\w+\\b', 'TC <TC>', generated)\n",
    "    \n",
    "    golden_Sexp = re.sub(r'GC \\w+\\b', 'GC <GC>', golden_Sexp)\n",
    "    golden_Sexp = re.sub(r'TC \\w+\\b', 'TC <TC>', golden_Sexp)\n",
    "\n",
    "    if predicted_Table == golden_table:\n",
    "        generatedmeasure = sexpgen.split(\"MSR\")[1].split(\" \")[1]\n",
    "            \n",
    "        if golden_measure == generatedmeasure:\n",
    "            # counter += 1\n",
    "            matches_generated = re.findall(pattern, generated)\n",
    "            matches_golden = re.findall(pattern, golden_Sexp)\n",
    "\n",
    "            ## matches generated is a list of tuples just make into one list continaing the elements of the tuples\n",
    "            matches_generated = sorted([item for sublist in matches_generated for item in sublist])\n",
    "            matches_golden = sorted([item for sublist in matches_golden for item in sublist])\n",
    "\n",
    "            if matches_generated == matches_golden:\n",
    "                counter += 1\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_correct['predicted_Tables'] = table_ids\n",
    "print(\"The table score is\")\n",
    "counter / len(df_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenAI Relevancy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import json\n",
    "import openai\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from openai import OpenAI\n",
    "openai.api_key = \"ADD YOUR API KEY\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"ADD YOUR API KEY\"\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def exact_openai_token_count(text):\n",
    "\t# Initialize the GPT-2 tokenizer\n",
    "\ttokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\t# Tokenize the text and count the number of tokens\n",
    "\ttokens = tokenizer.encode(text)\n",
    "\n",
    "\treturn len(tokens)\n",
    "\n",
    "\n",
    "\n",
    "def prompt_chatgpt(doc, model=\"gpt-3.5-turbo-0125\"):\n",
    "\n",
    "\n",
    "\ttemplate = f\"\"\"\n",
    "\tIn response to the query: '{doc}', \n",
    "\n",
    "\t- \"RelevancyScore\": A float between 0.0, and 1.0 indicating how well the table, measures, and dimensions match the query. A score of 1.0 indicates a perfect match.E valuate how well the table description fits the query. give a grade in the range, 0.0 for very bad, 0.5 for okay, 1.0 for perfect. You can ignore geographical locations and dates \n",
    "\n",
    "\n",
    "\t\"\"\"\n",
    "\n",
    "\n",
    "\tinput_tokens = exact_openai_token_count(template)\n",
    "\tinput_cost = input_tokens / 1000 * 0.0005   # Cost per 1K tokens for input\n",
    "\n",
    "\t\n",
    "\tresponse = client.chat.completions.create(\n",
    "\t\tmodel=model,    \n",
    "\t\tresponse_format={\"type\": \"json_object\"},\n",
    "\t\tmessages=[\n",
    "\t\t\t{\"role\": \"system\", \"content\": \"  A Table question and answering system retrieved a table cell with a certain description, you are tasked to evaluate how well the retrieved table cell fits the input query. You return a grade in a json format.\"},\n",
    "\t\t\t{\"role\": \"user\", \"content\": template}\n",
    "\t\t],\n",
    "\t\ttemperature=0.0,\n",
    "\n",
    "\t)\n",
    "\toutput = json.loads(response.choices[0].message.content)\n",
    "\toutput_tokens = exact_openai_token_count(f\"{output}\")\n",
    "\toutput_cost = output_tokens / 1000 * 0.0015  # Cost per 1K tokens for output\n",
    "\n",
    "\t# Calculate the total cost by adding input and output costs\n",
    "\ttotal_cost = input_cost + output_cost\n",
    "\n",
    "\t# # Print the costs\n",
    "\t# print(f\"Input tokens: {input_tokens}, Cost: ${input_cost:.4f}\")\n",
    "\t# print(f\"Output tokens: {output_tokens}, Cost: ${output_cost:.4f}\")\n",
    "\t# print(f\"Total cost: ${total_cost:.4f}\")\n",
    "\treturn output, total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import utils.utils as util\n",
    "costs, outputs = [], []\n",
    "for index, row in tqdm(df_correct[:1].iterrows()):\n",
    "    try:\n",
    "        pattern = r\"\\(DIM\\s+([^\\s]+)\\s+([A-Z0-9_]+)\\)\"\n",
    "\n",
    "        prompts = []\n",
    "\n",
    "        sexp = row['sexpgen']\n",
    "        query = row['query']\n",
    "\n",
    "        measure = sexp.split(\"MSR\")[1].split(\" \")[1]\n",
    "            \n",
    "        dimensions = re.findall(pattern, sexp)\n",
    "            \n",
    "        prompt = util.expressiontoprompt(sexp, measure, dimensions)\n",
    "            \n",
    "        string = f\"This is the Query: {query} \\n This is the prompt: {prompt}\"\n",
    "        # string += \"Evaluate how well the prompt fits the query. give a grade, 0.0 for very bad, 0.5 for okay, 1.0 for perfect. You can ignore geographical locations and dates.\"\n",
    "\n",
    "        output, cost = prompt_chatgpt(string)\n",
    "        # print(output)\n",
    "        # print(cost)\n",
    "\n",
    "        costs.append(cost)\n",
    "        outputs.append(output)\n",
    "    except:\n",
    "        costs.append(0)\n",
    "        outputs.append(\"Error\")\n",
    "\n",
    "df_correct['costs'] = costs\n",
    "df_correct['outputs'] = outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
